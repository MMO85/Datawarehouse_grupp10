# Datawarehouse_grupp10
HR pipeline
ðŸ“Š HR Analytics PoC â€” Modern Data Stack

Detta projekt Ã¤r ett Proof of Concept, dÃ¤r vi bygger en pipeline fÃ¶r att hÃ¤mta, transformera och visualisera jobbannonser frÃ¥n Jobtech API med hjÃ¤lp av en modern data stack.

ðŸ› ï¸ Teknikstack

Snowflake â€“ Data warehouse

dlt â€“ Data ingestion frÃ¥n Jobtech API

dbt â€“ Transformationer (staging â†’ warehouse â†’ mart)

Streamlit â€“ Dashboard med KPI:er

GitHub Actions â€“ CI fÃ¶r dbt


ðŸ‘¥ Teamroller


ðŸ”— FlÃ¶de: DLT â†’ DBT â†’ Streamlit
flowchart TD
    A[Jobtech API] -->|DLT pipeline| B[Snowflake STAGING]
    B -->|dbt models| C[Snowflake WAREHOUSE]
    C -->|dbt marts| D[Snowflake MART]
    D -->|SQL queries| E[Streamlit Dashboard]

ðŸš€ FlÃ¶de steg-fÃ¶r-steg
âš¡ DLT (Data Loading Tool)

Skript: dlt_pipeline/loadjob_ads.py

HÃ¤mtar jobbannonser frÃ¥n Jobtech Search API.

StÃ¶der occupation_field som filter + pagination.!

Laddar data till Snowflake â†’ HR_JOBS.STAGING.job_ads.

âš¡ DBT (Data Build Tool)

Staging â†’ stg_job_ads.sql

Rensar och standardiserar kolumner.

Warehouse â†’

fact_job_ads.sql (fakta-tabell)

dim_location.sql (dimension-tabell fÃ¶r stÃ¤der)

Mart â†’

mart_job_ads_by_field.sql (aggregeringar per yrkesfÃ¤lt).

Tester i schema.yml:

not_null, unique, accepted_values, relationships.

Dokumentation genereras med dbt docs.

âš¡ Streamlit

Dashboard: streamlit_app/app.py

Kopplar till Snowflake (MART-schema).

Visar KPI:er:

Totalt antal annonser

Topp 10 yrken

Topp 10 stÃ¤der

FÃ¶rdelning av anstÃ¤llningstyper

Interaktiv meny fÃ¶r att vÃ¤lja occupation_field.

ðŸ“‚ Projektstruktur
â”œâ”€â”€ dlt_pipeline/          # dlt pipelines
â”‚   â””â”€â”€ load_job_ads.py
â”œâ”€â”€ dbt_project/           # dbt-projekt (staging â†’ warehouse â†’ mart)
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ staging/
â”‚   â”‚   â”œâ”€â”€ warehouse/
â”‚   â”‚   â””â”€â”€ mart/
â”‚   â””â”€â”€ profiles.yml.example
â”œâ”€â”€ streamlit_app/         # Dashboard
â”‚   â””â”€â”€ app.py
â”œâ”€â”€ infra/                 # Infrastruktur
â”‚   â””â”€â”€ snowflake_setup.sql
â”œâ”€â”€ .github/workflows/     # CI/CD
â”‚   â””â”€â”€ ci.yml
â”œâ”€â”€ docs/                  # Dokumentation & checklist
â”œâ”€â”€ .env.example
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md

ðŸ”‘ Installation & kÃ¶rning
1. Klona repo och installera beroenden
git clone <repo-url>
cd hr-analytics-poc
python -m venv .venv
source .venv/bin/activate    # Windows: .venv\Scripts\Activate.ps1
pip install -r requirements.txt

2. MiljÃ¶variabler

Kopiera .env.example â†’ .env och fyll i:

Snowflake credentials

API-instÃ¤llningar fÃ¶r Jobtech

3. Snowflake-setup

KÃ¶r SQL i infra/snowflake_setup.sql fÃ¶r att skapa:

Warehouse

Database & Schemas (STAGING, WAREHOUSE, MART)

Roller & AnvÃ¤ndare

4. Ladda data (dlt)
python dlt_pipeline/load_job_ads.py

5. KÃ¶r dbt
cd dbt_project
dbt deps
dbt debug
dbt run
dbt test

6. KÃ¶r dashboard
streamlit run streamlit_app/app.py

âœ… Bonusar
ðŸ”¹ Task 5 â€” dbt dokumentation & datakvalitet

Extra tester (accepted_values, relationships).

Beskrivningar i schema.yml.

dbt docs generate + dbt docs serve fÃ¶r att visa lineage.

ðŸ”¹ Task 6 â€” Orkestrering (Dagster)

Pipeline som kÃ¶r ingestion (dlt) + transformation (dbt).

KÃ¶rs tvÃ¥ gÃ¥nger fÃ¶r att analysera trender i data (ex. jÃ¤mfÃ¶ra antal annonser dag 1 vs dag 2).

ðŸ“ˆ Resultat

Full pipeline: Jobtech API â†’ Snowflake â†’ dbt â†’ Streamlit.

Dashboarden visar meningsfulla KPI:er fÃ¶r HR-analys.

Datakvalitet sÃ¤kras med dbt-tests.

CI via GitHub Actions kÃ¶r dbt compile + tests pÃ¥ varje PR.

(Bonus) Dagster pipeline fÃ¶r orkestrering.
